{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c4ad6591-db8e-4901-97ae-1027e8505a0c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Ground-SAM Locally and Inpaint using Stable Diffusion Huggingface Diffuser"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e5d010bb-59e8-4077-8fab-ca377759ffdb",
   "metadata": {},
   "source": [
    "#### Initial Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e8536b-269f-45ed-9f5b-1d7746cf0d70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade sagemaker boto3\n",
    "!git clone https://github.com/IDEA-Research/Grounded-Segment-Anything.git\n",
    "%cd Grounded-Segment-Anything\n",
    "%mkdir dino_input_single\n",
    "%mkdir dino_output_single\n",
    "%cp ../test.jpg dino_input_single\n",
    "test_image_path = 'dino_input_single/test.jpg'\n",
    "test_mask_path = 'dino_output_single/test_mask.png'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4a1f8d19-ae18-4f7e-96f6-4ecedb87859f",
   "metadata": {},
   "source": [
    "# Step1: Using Prompts to Detect Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd95728-cbea-4316-93ed-20f94c88e99c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%cd ./GroundingDINO/\n",
    "%pip install -e ."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bb746f9f-4e87-41dc-9475-32a0cce0ce77",
   "metadata": {},
   "source": [
    "#### Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3285bdc4-15f9-414a-946b-a7257e6f2035",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import supervision as sv\n",
    "\n",
    "import argparse\n",
    "from functools import partial\n",
    "import cv2\n",
    "import requests\n",
    "\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "from torchvision.ops import box_convert\n",
    "\n",
    "from groundingdino.models import build_model\n",
    "from groundingdino.util.slconfig import SLConfig\n",
    "from groundingdino.util.utils import clean_state_dict\n",
    "from groundingdino.util.inference import annotate, load_image, predict\n",
    "import groundingdino.datasets.transforms as T\n",
    "\n",
    "from huggingface_hub import hf_hub_download"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e1f15a22-bfe9-48d8-8b8c-8e58c8827ca3",
   "metadata": {},
   "source": [
    "#### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706f3ad3-7116-45a5-aadb-b13b1a487eab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_model_hf(repo_id, filename, ckpt_config_filename, device='cpu'):\n",
    "    cache_config_file = hf_hub_download(repo_id=repo_id, filename=ckpt_config_filename)\n",
    "\n",
    "    args = SLConfig.fromfile(cache_config_file) \n",
    "    model = build_model(args)\n",
    "    args.device = device\n",
    "\n",
    "    cache_file = hf_hub_download(repo_id=repo_id, filename=filename)\n",
    "    checkpoint = torch.load(cache_file, map_location='cpu')\n",
    "    log = model.load_state_dict(clean_state_dict(checkpoint['model']), strict=False)\n",
    "    print(\"Model loaded from {} \\n => {}\".format(cache_file, log))\n",
    "    _ = model.eval()\n",
    "    return model    \n",
    "\n",
    "def generate_masks_with_grounding(image_source, boxes):\n",
    "    box_list = []\n",
    "    h, w, _ = image_source.shape\n",
    "    boxes_unnorm = boxes * torch.Tensor([w, h, w, h])\n",
    "    boxes_xyxy = box_convert(boxes=boxes_unnorm, in_fmt=\"cxcywh\", out_fmt=\"xyxy\").numpy()\n",
    "    mask = np.zeros_like(image_source)\n",
    "    for box in boxes_xyxy:\n",
    "        x0, y0, x1, y1 = box\n",
    "        box_list.append(np.array([int(x0),int(y0),int(x1),int(y1)]))\n",
    "        mask[int(y0):int(y1), int(x0):int(x1), :] = 255\n",
    "    return mask, box_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4c8cd0d0-7856-47b2-9a54-54d98dea8511",
   "metadata": {},
   "source": [
    "#### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f9349b-40e2-4598-a2d9-3ec2538e7e3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use this command for evaluate the Grounding DINO model\n",
    "# Or you can download the model by yourself\n",
    "ckpt_repo_id = \"ShilongLiu/GroundingDINO\"\n",
    "ckpt_filenmae = \"groundingdino_swint_ogc.pth\"\n",
    "ckpt_config_filename = \"GroundingDINO_SwinT_OGC.cfg.py\"\n",
    "model = load_model_hf(ckpt_repo_id, ckpt_filenmae, ckpt_config_filename)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d4786041-88ac-4720-8dbd-15d72198a085",
   "metadata": {},
   "source": [
    "#### Generate detection boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30f7276-3062-4420-a5f9-b2835d76a85c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_image_path=\"../dino_input_single/test.jpg\"\n",
    "\n",
    "TEXT_PROMPT = \"white dress with blue patterns\"\n",
    "BOX_TRESHOLD = 0.5\n",
    "TEXT_TRESHOLD = 0.5\n",
    "\n",
    "image_source, image = load_image(test_image_path)\n",
    "boxes, logits, phrases = predict(\n",
    "    model=model, \n",
    "    image=image, \n",
    "    caption=TEXT_PROMPT, \n",
    "    box_threshold=BOX_TRESHOLD, \n",
    "    text_threshold=TEXT_TRESHOLD\n",
    ")\n",
    "\n",
    "annotated_frame = annotate(image_source=image_source, boxes=boxes, logits=logits, phrases=phrases)\n",
    "annotated_frame = annotated_frame[...,::-1] # BGR to RGB\n",
    "image_mask, box_list = generate_masks_with_grounding(image_source, boxes)\n",
    "## Get the detection boxes\n",
    "## For simplicity, here we are only using the first box, where ideally you can iter through all the boxes detected\n",
    "dino_box = box_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8403292-93f5-4c89-b99f-67ec45f0efae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Show detections\n",
    "Image.fromarray(annotated_frame)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eadfe9b2-44b4-40c3-bb6a-870ed4b10d39",
   "metadata": {},
   "source": [
    "# Step2: Generate Segmentation with Detection Boxes as Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975a2b8a-f124-472d-ad7d-cf2c9d9fff9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%cd ../segment_anything/\n",
    "%pip install -e .\n",
    "## download the segment model\n",
    "!wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\n",
    "%pip install diffusers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "32b4dcd9-b7a1-4b90-bb5e-d3430372cd0e",
   "metadata": {},
   "source": [
    "#### Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75714f7-c6de-40e8-a278-28a0d5aa659d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "from PIL import Image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dc3d0d3e-6d9b-4c03-ba25-08c8fdb997f3",
   "metadata": {},
   "source": [
    "#### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acd8567-6d45-4750-8065-771f24997592",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def show_mask(mask, ax, random_color=False):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)\n",
    "    \n",
    "def show_points(coords, labels, ax, marker_size=375):\n",
    "    pos_points = coords[labels==1]\n",
    "    neg_points = coords[labels==0]\n",
    "    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
    "    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)   \n",
    "    \n",
    "def show_box(box, ax):\n",
    "    x0, y0 = box[0], box[1]\n",
    "    w, h = box[2] - box[0], box[3] - box[1]\n",
    "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0,0,0,0), lw=2))  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a3e52b54-0a97-4eb8-95f3-540e8bb7c218",
   "metadata": {},
   "source": [
    "#### Generate segmentation masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd0cf51-a635-4de9-95a1-3e524192321f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_image_path = '../dino_input_single/test.jpg'\n",
    "test_mask_path = '../dino_output_single/test_mask.png'\n",
    "\n",
    "## Loading model\n",
    "sam_checkpoint = \"sam_vit_h_4b8939.pth\"\n",
    "model_type = \"vit_h\"\n",
    "device = \"cuda\"\n",
    "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
    "sam.to(device=device)\n",
    "## Inference\n",
    "predictor = SamPredictor(sam)\n",
    "image_pil_arry = np.array(Image.open(test_image_path).convert('RGB'))\n",
    "predictor.set_image(image_pil_arry)\n",
    "masks, _, _ = predictor.predict(\n",
    "    box=dino_box[None, :],\n",
    "    multimask_output=False,\n",
    ")\n",
    "## save the mask\n",
    "Image.fromarray(masks[0]).save(test_mask_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87736ae-ef10-439a-90f6-a8fce659c5e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(image_pil_arry )\n",
    "show_mask(masks[0], plt.gca())\n",
    "#show_box(dino_box, plt.gca())\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ce5b85ae-ea10-42ea-a1d3-6d4467789706",
   "metadata": {},
   "source": [
    "# Step3: Bonus Inpainting"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "780f5840-f955-4bd6-b026-2eddfa83eb9f",
   "metadata": {},
   "source": [
    "#### Loading libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222b7702-dca0-4401-a943-0d876c70ba7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# diffusers\n",
    "import torch\n",
    "from diffusers import StableDiffusionInpaintPipeline\n",
    "from diffusers import DDIMScheduler"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "70c0f9df-3b96-4b0a-9e57-bf7f49a4c07a",
   "metadata": {},
   "source": [
    "#### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3949c8f-aa38-4d7d-ab70-199055b2665e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def image_read(image_file):\n",
    "    return Image.open(image_file).convert(\"RGB\")\n",
    "\n",
    "def mask_read(mask_file):\n",
    "    return Image.open(mask_file).convert('1') \n",
    "    \n",
    "def image_fuser(new_image, org_image, mask):\n",
    "    new_image = np.array(new_image)\n",
    "    org_image = np.array(org_image)\n",
    "    mask = np.array(mask)\n",
    "    org_image[mask] = new_image[mask]\n",
    "    result = Image.fromarray(org_image)\n",
    "    return result\n",
    "\n",
    "def generate_image(image, mask, prompt, negative_prompt, pipe, seed):\n",
    "    # resize for inpainting \n",
    "    w, h = image.size\n",
    "    in_image = image.resize((512, 512))\n",
    "    in_mask = mask.resize((512, 512))\n",
    "    generator = torch.Generator(device).manual_seed(seed) \n",
    "    image_gen = pipe(image=in_image, mask_image=in_mask, prompt=prompt, negative_prompt=negative_prompt, generator=generator)\n",
    "    image_gen = image_gen.images[0]\n",
    "    result = image_fuser(image_gen, in_image, in_mask)\n",
    "    result = result.resize((w, h))\n",
    "    return result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "32ff2c26-958a-4f64-9270-c2d835e8be0a",
   "metadata": {},
   "source": [
    "#### Generating images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20642ede-2044-4256-bd03-1dd9412ca712",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Loading and config model\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "pipe = StableDiffusionInpaintPipeline.from_pretrained(\"stabilityai/stable-diffusion-2-inpainting\", torch_dtype=torch.float16, safety_checker = None)\n",
    "pipe = pipe.to('cuda')\n",
    "pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)\n",
    "## Inference\n",
    "inpaint_prompt = 'an extremely beautiful dreamy white lace cotton dress with delicate see-through sleeves, extra detailes, (high detailed skin:1.2), 8k uhd, dslr, soft lighting, high quality, film grain, Fujifilm XT3, photorealistic'\n",
    "inpaint_negative_prompt=\"(deformed iris, deformed pupils, semi-realistic, cgi, 3d, render, sketch, cartoon, drawing, anime:1.4), text, close up, cropped, out of frame, worst quality, low quality, jpeg artifacts, ugly, duplicate, morbid, mutilated, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, mutation, deformed, blurry, dehydrated, bad anatomy, bad proportions, extra limbs, cloned face, disfigured, gross proportions, malformed limbs, missing arms, missing legs, extra arms, extra legs, fused fingers, too many fingers, long neck\"\n",
    "seed = 512 # for reproducibility \n",
    "guidance_scale = 7.5\n",
    "num_inference_steps = 50\n",
    "org_image = image_read(test_image_path)\n",
    "seg_mask = mask_read(test_mask_path)\n",
    "inpainted_image = generate_image(org_image, seg_mask, inpaint_prompt, inpaint_negative_prompt, pipe, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd725da1-1762-4b7c-bc41-bd366b29369e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inpainted_image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2a4a61-7ba1-43a7-8f0c-56a8f7ec31af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p39",
   "language": "python",
   "name": "conda_pytorch_p39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
